{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning the library models for language modeling on WikiText-2 (GPT, GPT-2, BERT, RoBERTa).\n",
    "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
    "using a masked language modeling (MLM) loss.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "    \n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup,\n",
    "                          GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)}\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokenizer, file_path='train', block_size=512):\n",
    "        assert os.path.isfile(file_path)\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(directory, f'cached_lm_{block_size}_{filename}')\n",
    "\n",
    "        if os.path.exists(cached_features_file):\n",
    "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, 'rb') as handle:\n",
    "                self.examples = pickle.load(handle)\n",
    "        else:\n",
    "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
    "\n",
    "            self.examples = []\n",
    "            with open(file_path, encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
    "\n",
    "            for i in range(0, len(tokenized_text)-block_size+1, block_size): # Truncate in block of block_size\n",
    "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
    "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
    "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
    "            # can change this behavior by adding (model specific) padding.\n",
    "\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            with open(cached_features_file, 'wb') as handle:\n",
    "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.examples[item])\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
    "    dataset = TextDataset(tokenizer, file_path=args['eval_data_file'] if evaluate else args['train_data_file'], block_size=args['block_size'])\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    if args['n_gpu'] > 0:\n",
    "        torch.cuda.manual_seed_all(args['seed'])\n",
    "\n",
    "\n",
    "def mask_tokens(inputs, tokenizer, args):\n",
    "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
    "    masked_indices = torch.bernoulli(torch.full(labels.shape, args['mlm_probability'])).bool()\n",
    "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args['local_rank'] in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args['train_batch_size'] = args['per_gpu_train_batch_size'] * max(1, args['n_gpu'])\n",
    "    train_sampler = RandomSampler(train_dataset) if args['local_rank'] == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "\n",
    "    if args['max_steps'] > 0:\n",
    "        t_total = args['max_steps']\n",
    "        args['num_train_epochs'] = args['max_steps'] // (len(train_dataloader) // args['gradient_accumulation_steps']) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "#     scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps = t_total)\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args['n_gpu'] > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args['local_rank'] != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args['local_rank']],\n",
    "                                                          output_device=args['local_rank'],\n",
    "                                                          find_unused_parameters=True)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args['per_gpu_train_batch_size'])\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                   args['train_batch_size'] * args['gradient_accumulation_steps'] * (torch.distributed.get_world_size() if args['local_rank'] != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\", disable=args['local_rank'] not in [-1, 0])\n",
    "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args['local_rank'] not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            inputs, labels = mask_tokens(batch, tokenizer, args) if args['mlm'] else (batch, batch)\n",
    "            inputs = inputs.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            model.train()\n",
    "            outputs = model(inputs, masked_lm_labels=labels) if args['mlm'] else model(inputs, labels=labels)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "\n",
    "            if args['n_gpu'] > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                if args['fp16']:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args['local_rank'] in [-1, 0] and args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    if args['local_rank'] == -1 and args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['local_rank'] in [-1, 0] and args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args['local_rank'] in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "\n",
    "    results = {}\n",
    "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
    "\n",
    "    if not os.path.exists(eval_output_dir) and args['local_rank'] in [-1, 0]:\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    args['eval_batch_size'] = args['per_gpu_eval_batch_size'] * max(1, args['n_gpu'])\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset) if args['local_rank'] == -1 else DistributedSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = batch.to(args['device'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, masked_lm_labels=batch) if args['mlm'] else model(batch, labels=batch)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\n",
    "        \"perplexity\": perplexity\n",
    "    }\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    if args['model_type'] in [\"bert\", \"roberta\"] and not args['mlm']:\n",
    "        raise ValueError(\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
    "                         \"flag (masked language modeling).\")\n",
    "    if args['eval_data_file'] is None and args['do_eval']:\n",
    "        raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
    "                         \"or remove the --do_eval argument.\")\n",
    "\n",
    "    if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))\n",
    "\n",
    "    # Setup distant debugging if needed\n",
    "    if args['server_ip'] and args['server_port']:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args['server_ip'], args['server_port']), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    if args['local_rank'] == -1 or args['no_cuda']:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args['no_cuda'] else \"cpu\")\n",
    "        args['n_gpu'] = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args['local_rank'])\n",
    "        device = torch.device(\"cuda\", args['local_rank'])\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args['n_gpu'] = 1\n",
    "    args['device'] = device\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO if args['local_rank'] in [-1, 0] else logging.WARN)\n",
    "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "                    args['local_rank'], device, args['n_gpu'], bool(args['local_rank'] != -1), args['fp16'])\n",
    "\n",
    "    # Set seed\n",
    "    set_seed(args)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    if args['local_rank'] not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "    config = config_class.from_pretrained(args['config_name'] if args['config_name'] else args['model_name_or_path'])\n",
    "\n",
    "    #for new models\n",
    "    tokenizer = tokenizer_class.from_pretrained(args['tokenizer_name'] if args['tokenizer_name'] else args['model_name_or_path'], do_lower_case=args['do_lower_case'])\n",
    "\n",
    "#    might be needed if loading a finetuned model?\n",
    "#    tokenizer = tokenizer_class.from_pretrained(args['output_dir'], do_lower_case=args['do_lower_case'])\n",
    "    \n",
    "    if args['block_size'] <= 0:\n",
    "#         args['block_size'] = tokenizer.max_len  # Our input block size will be the max possible for the model\n",
    "        args['block_size'] = tokenizer.max_len_single_sentence\n",
    "    args['block_size'] = min(args['block_size'], tokenizer.max_len_single_sentence)\n",
    "\n",
    "#    for new models\n",
    "    model = model_class.from_pretrained(args['model_name_or_path'], from_tf=bool('.ckpt' in args['model_name_or_path']), config=config)\n",
    "\n",
    "#   for finetuned models    \n",
    "#    model = model_class.from_pretrained(args['output_dir'])\n",
    "    model.to(args['device'])\n",
    "\n",
    "    if args['local_rank'] == 0:\n",
    "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
    "\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args['do_train']:\n",
    "        if args['local_rank'] not in [-1, 0]:\n",
    "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
    "\n",
    "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
    "\n",
    "        if args['local_rank'] == 0:\n",
    "            torch.distributed.barrier()\n",
    "\n",
    "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "\n",
    "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
    "    if args['do_train'] and (args['local_rank'] == -1 or torch.distributed.get_rank() == 0):\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args['output_dir']) and args['local_rank'] in [-1, 0]:\n",
    "            os.makedirs(args['output_dir'])\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "        # They can then be reloaded using `from_pretrained()`\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(args['output_dir'])\n",
    "        tokenizer.save_pretrained(args['output_dir'])\n",
    "\n",
    "        # Good practice: save your training arguments together with the trained model\n",
    "        torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))\n",
    "\n",
    "        # Load a trained model and vocabulary that you have fine-tuned\n",
    "        model = model_class.from_pretrained(args['output_dir'])\n",
    "        tokenizer = tokenizer_class.from_pretrained(args['output_dir'], do_lower_case=args['do_lower_case'])\n",
    "        model.to(args['device'])\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if args['do_eval'] and args['local_rank'] in [-1, 0]:\n",
    "        checkpoints = [args['output_dir']]\n",
    "        if args['eval_all_checkpoints']:\n",
    "            checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args['output_dir'] + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "        for checkpoint in checkpoints:\n",
    "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "            model = model_class.from_pretrained(checkpoint)\n",
    "            model.to(args['device'])\n",
    "            result = evaluate(args, model, tokenizer, prefix=global_step)\n",
    "            result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "            results.update(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "tokenizer = tokenizer_class.from_pretrained(args['tokenizer_name'] if args['tokenizer_name'] else args['model_name_or_path'], do_lower_case=args['do_lower_case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "## Required parameters\n",
    "args['train_data_file']='IconicShoesTrain.txt'\n",
    "\n",
    "#if loading a checkpoint, make 'output_dir' the checkpoint folder itself, not the overall output folder'\n",
    "args['output_dir']='IconicShoesOutput'\n",
    "args['eval_data_file']='IconicShoesEval.txt'\n",
    "\n",
    "args['model_type']=\"gpt2\"\n",
    "args['model_name_or_path']=\"gpt2\"\n",
    "#args['model_name_or_path']=\"HF5000Output/checkpoint7\"\n",
    "\n",
    "args['mlm']=False\n",
    "args['mlm_probability']=0.15\n",
    "\n",
    "args['config_name']=\"\"\n",
    "args['tokenizer_name']=\"gpt2\"\n",
    "args['cache_dir']=\"\"\n",
    "args['block_size']=-1\n",
    "args['do_train']=True\n",
    "args['do_eval']=True\n",
    "args['evaluate_during_training']=True\n",
    "args['do_lower_case']=True\n",
    "\n",
    "args['per_gpu_train_batch_size']=1\n",
    "args['per_gpu_eval_batch_size']=1\n",
    "args['gradient_accumulation_steps']=1\n",
    "args['learning_rate']=5e-5\n",
    "args['weight_decay']=0.0\n",
    "args['adam_epsilon']=1e-8\n",
    "args['max_grad_norm']=1.0\n",
    "args['num_train_epochs']=300.0\n",
    "args['max_steps']=-1\n",
    "args['warmup_steps']=0\n",
    "\n",
    "args['logging_steps']=20000\n",
    "args['save_steps']=5000\n",
    "args['eval_all_checkpoints']=False\n",
    "args['no_cuda']=False\n",
    "args['overwrite_output_dir']=True\n",
    "args['overwrite_cache']=True\n",
    "args['seed']=42\n",
    "args['fp16']=True\n",
    "args['fp16_opt_level']='O1'\n",
    "args['local_rank']=-1\n",
    "args['server_ip']=''\n",
    "args['server_port']=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2020 20:42:03 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "02/05/2020 20:42:03 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/jupyter/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.699bbd1c449e9861456f359d6daa51bd523ac085b4b531ab0aad5a55d091e942\n",
      "02/05/2020 20:42:03 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:42:05 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/jupyter/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "02/05/2020 20:42:05 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/jupyter/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "02/05/2020 20:42:06 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/jupyter/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "02/05/2020 20:42:12 - INFO - __main__ -   Training/evaluation parameters {'train_data_file': 'IconicShoesTrain.txt', 'output_dir': 'IconicShoesOutput', 'eval_data_file': 'IconicShoesEval.txt', 'model_type': 'gpt2', 'model_name_or_path': 'gpt2', 'mlm': False, 'mlm_probability': 0.15, 'config_name': '', 'tokenizer_name': 'gpt2', 'cache_dir': '', 'block_size': 1024, 'do_train': False, 'do_eval': True, 'evaluate_during_training': True, 'do_lower_case': True, 'per_gpu_train_batch_size': 1, 'per_gpu_eval_batch_size': 1, 'gradient_accumulation_steps': 1, 'learning_rate': 5e-05, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'max_grad_norm': 1.0, 'num_train_epochs': 300.0, 'max_steps': -1, 'warmup_steps': 0, 'logging_steps': 20000, 'save_steps': 5000, 'eval_all_checkpoints': True, 'no_cuda': False, 'overwrite_output_dir': True, 'overwrite_cache': True, 'seed': 42, 'fp16': True, 'fp16_opt_level': 'O1', 'local_rank': -1, 'server_ip': '', 'server_port': '', 'n_gpu': 1, 'device': device(type='cuda')}\n",
      "02/05/2020 20:42:12 - INFO - __main__ -   Evaluate the following checkpoints: ['IconicShoesOutput/checkpoint-1', 'IconicShoesOutput/checkpoint-15', 'IconicShoesOutput/checkpoint-2', 'IconicShoesOutput/checkpoint-25', 'IconicShoesOutput/checkpoint-30000', 'IconicShoesOutput/checkpoint-35000', 'IconicShoesOutput/checkpoint-40000', 'IconicShoesOutput/checkpoint-45000', 'IconicShoesOutput/checkpoint-5000', 'IconicShoesOutput/checkpoint-50000', 'IconicShoesOutput/checkpoint-55000', 'IconicShoesOutput/checkpoint-60000', 'IconicShoesOutput/checkpoint-65000', 'IconicShoesOutput/checkpoint-70000', 'IconicShoesOutput/checkpoint-75000', 'IconicShoesOutput/checkpoint-80000', 'IconicShoesOutput/checkpoint-85000', 'IconicShoesOutput/checkpoint-90000', 'IconicShoesOutput/checkpoint-95000']\n",
      "02/05/2020 20:42:12 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-1/config.json\n",
      "02/05/2020 20:42:12 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:42:12 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-1/pytorch_model.bin\n",
      "02/05/2020 20:42:15 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:42:15 - INFO - __main__ -   ***** Running evaluation 1 *****\n",
      "02/05/2020 20:42:15 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:42:15 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.59it/s]\n",
      "02/05/2020 20:42:27 - INFO - __main__ -   ***** Eval results 1 *****\n",
      "02/05/2020 20:42:27 - INFO - __main__ -     perplexity = tensor(3.4344)\n",
      "02/05/2020 20:42:27 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-15/config.json\n",
      "02/05/2020 20:42:27 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:42:27 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-15/pytorch_model.bin\n",
      "02/05/2020 20:42:30 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:42:30 - INFO - __main__ -   ***** Running evaluation 15 *****\n",
      "02/05/2020 20:42:30 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:42:30 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.79it/s]\n",
      "02/05/2020 20:42:42 - INFO - __main__ -   ***** Eval results 15 *****\n",
      "02/05/2020 20:42:42 - INFO - __main__ -     perplexity = tensor(3.5936)\n",
      "02/05/2020 20:42:42 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-2/config.json\n",
      "02/05/2020 20:42:42 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2020 20:42:42 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-2/pytorch_model.bin\n",
      "02/05/2020 20:42:45 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:42:45 - INFO - __main__ -   ***** Running evaluation 2 *****\n",
      "02/05/2020 20:42:45 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:42:45 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.80it/s]\n",
      "02/05/2020 20:42:57 - INFO - __main__ -   ***** Eval results 2 *****\n",
      "02/05/2020 20:42:57 - INFO - __main__ -     perplexity = tensor(3.8511)\n",
      "02/05/2020 20:42:57 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-25/config.json\n",
      "02/05/2020 20:42:57 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:42:57 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-25/pytorch_model.bin\n",
      "02/05/2020 20:43:00 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:43:00 - INFO - __main__ -   ***** Running evaluation 25 *****\n",
      "02/05/2020 20:43:00 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:43:00 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.81it/s]\n",
      "02/05/2020 20:43:12 - INFO - __main__ -   ***** Eval results 25 *****\n",
      "02/05/2020 20:43:12 - INFO - __main__ -     perplexity = tensor(4.1340)\n",
      "02/05/2020 20:43:12 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-30000/config.json\n",
      "02/05/2020 20:43:12 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:43:12 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-30000/pytorch_model.bin\n",
      "02/05/2020 20:43:15 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:43:15 - INFO - __main__ -   ***** Running evaluation 30000 *****\n",
      "02/05/2020 20:43:15 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:43:15 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.81it/s]\n",
      "02/05/2020 20:43:27 - INFO - __main__ -   ***** Eval results 30000 *****\n",
      "02/05/2020 20:43:27 - INFO - __main__ -     perplexity = tensor(4.4747)\n",
      "02/05/2020 20:43:27 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-35000/config.json\n",
      "02/05/2020 20:43:27 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:43:27 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-35000/pytorch_model.bin\n",
      "02/05/2020 20:43:30 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:43:30 - INFO - __main__ -   ***** Running evaluation 35000 *****\n",
      "02/05/2020 20:43:30 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:43:30 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:43:41 - INFO - __main__ -   ***** Eval results 35000 *****\n",
      "02/05/2020 20:43:41 - INFO - __main__ -     perplexity = tensor(4.8018)\n",
      "02/05/2020 20:43:41 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-40000/config.json\n",
      "02/05/2020 20:43:41 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:43:41 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-40000/pytorch_model.bin\n",
      "02/05/2020 20:43:45 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:43:45 - INFO - __main__ -   ***** Running evaluation 40000 *****\n",
      "02/05/2020 20:43:45 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:43:45 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.81it/s]\n",
      "02/05/2020 20:43:56 - INFO - __main__ -   ***** Eval results 40000 *****\n",
      "02/05/2020 20:43:56 - INFO - __main__ -     perplexity = tensor(5.1090)\n",
      "02/05/2020 20:43:56 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-45000/config.json\n",
      "02/05/2020 20:43:56 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2020 20:43:56 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-45000/pytorch_model.bin\n",
      "02/05/2020 20:44:00 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:44:00 - INFO - __main__ -   ***** Running evaluation 45000 *****\n",
      "02/05/2020 20:44:00 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:44:00 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:44:11 - INFO - __main__ -   ***** Eval results 45000 *****\n",
      "02/05/2020 20:44:11 - INFO - __main__ -     perplexity = tensor(5.4846)\n",
      "02/05/2020 20:44:11 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-5000/config.json\n",
      "02/05/2020 20:44:11 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:44:11 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-5000/pytorch_model.bin\n",
      "02/05/2020 20:44:15 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:44:15 - INFO - __main__ -   ***** Running evaluation 5000 *****\n",
      "02/05/2020 20:44:15 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:44:15 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.81it/s]\n",
      "02/05/2020 20:44:26 - INFO - __main__ -   ***** Eval results 5000 *****\n",
      "02/05/2020 20:44:26 - INFO - __main__ -     perplexity = tensor(3.6836)\n",
      "02/05/2020 20:44:26 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-50000/config.json\n",
      "02/05/2020 20:44:26 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:44:26 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-50000/pytorch_model.bin\n",
      "02/05/2020 20:44:29 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:44:29 - INFO - __main__ -   ***** Running evaluation 50000 *****\n",
      "02/05/2020 20:44:29 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:44:29 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:44:41 - INFO - __main__ -   ***** Eval results 50000 *****\n",
      "02/05/2020 20:44:41 - INFO - __main__ -     perplexity = tensor(5.7405)\n",
      "02/05/2020 20:44:41 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-55000/config.json\n",
      "02/05/2020 20:44:41 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:44:41 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-55000/pytorch_model.bin\n",
      "02/05/2020 20:44:44 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:44:44 - INFO - __main__ -   ***** Running evaluation 55000 *****\n",
      "02/05/2020 20:44:44 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:44:44 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.81it/s]\n",
      "02/05/2020 20:44:56 - INFO - __main__ -   ***** Eval results 55000 *****\n",
      "02/05/2020 20:44:56 - INFO - __main__ -     perplexity = tensor(6.0674)\n",
      "02/05/2020 20:44:56 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-60000/config.json\n",
      "02/05/2020 20:44:56 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:44:56 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-60000/pytorch_model.bin\n",
      "02/05/2020 20:44:59 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:44:59 - INFO - __main__ -   ***** Running evaluation 60000 *****\n",
      "02/05/2020 20:44:59 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:44:59 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:45:10 - INFO - __main__ -   ***** Eval results 60000 *****\n",
      "02/05/2020 20:45:10 - INFO - __main__ -     perplexity = tensor(6.2902)\n",
      "02/05/2020 20:45:10 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-65000/config.json\n",
      "02/05/2020 20:45:10 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2020 20:45:10 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-65000/pytorch_model.bin\n",
      "02/05/2020 20:45:14 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:45:14 - INFO - __main__ -   ***** Running evaluation 65000 *****\n",
      "02/05/2020 20:45:14 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:45:14 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.80it/s]\n",
      "02/05/2020 20:45:25 - INFO - __main__ -   ***** Eval results 65000 *****\n",
      "02/05/2020 20:45:25 - INFO - __main__ -     perplexity = tensor(6.4034)\n",
      "02/05/2020 20:45:25 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-70000/config.json\n",
      "02/05/2020 20:45:25 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:45:25 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-70000/pytorch_model.bin\n",
      "02/05/2020 20:45:28 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:45:28 - INFO - __main__ -   ***** Running evaluation 70000 *****\n",
      "02/05/2020 20:45:28 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:45:28 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:45:40 - INFO - __main__ -   ***** Eval results 70000 *****\n",
      "02/05/2020 20:45:40 - INFO - __main__ -     perplexity = tensor(6.6597)\n",
      "02/05/2020 20:45:40 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-75000/config.json\n",
      "02/05/2020 20:45:40 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:45:40 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-75000/pytorch_model.bin\n",
      "02/05/2020 20:45:43 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:45:43 - INFO - __main__ -   ***** Running evaluation 75000 *****\n",
      "02/05/2020 20:45:43 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:45:43 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:45:55 - INFO - __main__ -   ***** Eval results 75000 *****\n",
      "02/05/2020 20:45:55 - INFO - __main__ -     perplexity = tensor(6.8251)\n",
      "02/05/2020 20:45:55 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-80000/config.json\n",
      "02/05/2020 20:45:55 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:45:55 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-80000/pytorch_model.bin\n",
      "02/05/2020 20:45:58 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:45:58 - INFO - __main__ -   ***** Running evaluation 80000 *****\n",
      "02/05/2020 20:45:58 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:45:58 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.80it/s]\n",
      "02/05/2020 20:46:09 - INFO - __main__ -   ***** Eval results 80000 *****\n",
      "02/05/2020 20:46:09 - INFO - __main__ -     perplexity = tensor(6.9719)\n",
      "02/05/2020 20:46:09 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-85000/config.json\n",
      "02/05/2020 20:46:09 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:46:09 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-85000/pytorch_model.bin\n",
      "02/05/2020 20:46:13 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:46:13 - INFO - __main__ -   ***** Running evaluation 85000 *****\n",
      "02/05/2020 20:46:13 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:46:13 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.80it/s]\n",
      "02/05/2020 20:46:24 - INFO - __main__ -   ***** Eval results 85000 *****\n",
      "02/05/2020 20:46:24 - INFO - __main__ -     perplexity = tensor(7.0844)\n",
      "02/05/2020 20:46:24 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-90000/config.json\n",
      "02/05/2020 20:46:24 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2020 20:46:24 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-90000/pytorch_model.bin\n",
      "02/05/2020 20:46:27 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:46:27 - INFO - __main__ -   ***** Running evaluation 90000 *****\n",
      "02/05/2020 20:46:27 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:46:27 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.82it/s]\n",
      "02/05/2020 20:46:39 - INFO - __main__ -   ***** Eval results 90000 *****\n",
      "02/05/2020 20:46:39 - INFO - __main__ -     perplexity = tensor(7.2354)\n",
      "02/05/2020 20:46:39 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-95000/config.json\n",
      "02/05/2020 20:46:39 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 20:46:39 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-95000/pytorch_model.bin\n",
      "02/05/2020 20:46:42 - INFO - __main__ -   Loading features from cached file cached_lm_1024_IconicShoesEval.txt\n",
      "02/05/2020 20:46:42 - INFO - __main__ -   ***** Running evaluation 95000 *****\n",
      "02/05/2020 20:46:42 - INFO - __main__ -     Num examples = 170\n",
      "02/05/2020 20:46:42 - INFO - __main__ -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 170/170 [00:11<00:00, 14.81it/s]\n",
      "02/05/2020 20:46:53 - INFO - __main__ -   ***** Eval results 95000 *****\n",
      "02/05/2020 20:46:53 - INFO - __main__ -     perplexity = tensor(7.4615)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['do_train']=False\n",
    "args['eval_all_checkpoints']=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Conditional text generation with the auto-regressive models of the library (GPT/GPT-2/Transformer-XL/XLNet)\n",
    "\"\"\"\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from transformers import GPT2Config, OpenAIGPTConfig, XLNetConfig, TransfoXLConfig\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "ALL_MODELS = sum((tuple(conf.pretrained_config_archive_map.keys()) for conf in (GPT2Config,)), ())\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer)}\n",
    "\n",
    "def set_seed(args):\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    if args['n_gpu'] > 0:\n",
    "        torch.cuda.manual_seed_all(args['seed'])\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.9, is_xlnet=False, repetition_penalty=1.0, device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            \n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][0, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: #greedy sampling:\n",
    "                # Return indices of the top num_samples logits (i.e. equivalent to argmax if num_samples = 1)\n",
    "                next_token = torch.topk(filtered_logits, num_samples)[1]\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=num_samples, replacement=True)\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(1)), dim=1)\n",
    "    return generated\n",
    "                     \n",
    "\n",
    "def generate(args):   \n",
    "\n",
    "    args['device'] = torch.device(\"cuda\" if torch.cuda.is_available() and not args['no_cuda'] else \"cpu\")\n",
    "    args['n_gpu'] = torch.cuda.device_count()\n",
    "\n",
    "    set_seed(args)\n",
    "\n",
    "    args['model_type'] = args['model_type'].lower()\n",
    "    model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "    tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "    model = model_class.from_pretrained(args['model_name_or_path'])\n",
    "    model.to(args['device'])\n",
    "    model.eval()\n",
    "\n",
    "    if args['length'] < 0 and model.config.max_position_embeddings > 0:\n",
    "        args['length'] = model.config.max_position_embeddings\n",
    "    elif 0 < model.config.max_position_embeddings < args['length']:\n",
    "        args['length'] = model.config.max_position_embeddings  # No generation bigger than model size \n",
    "    elif args['length'] < 0:\n",
    "        args['length'] = MAX_LENGTH  # avoid infinite loop\n",
    "\n",
    "#    print(args)\n",
    "    \n",
    "\n",
    "    while True:\n",
    "        raw_text = args['prompt'] if args['prompt'] else input(\"Model prompt >>> \")\n",
    "\n",
    "        if args['model_type'] in [\"transfo-xl\", \"xlnet\"]:\n",
    "            # Models with memory likes to have a long prompt for short inputs.\n",
    "            raw_text = (args['padding_text'] if args['padding_text'] else PADDING_TEXT) + raw_text\n",
    "        context_tokens = tokenizer.encode(raw_text, add_special_tokens=False)\n",
    "        outputs = sample_sequence(\n",
    "            model=model,\n",
    "            context=context_tokens,\n",
    "            length=args['length'],\n",
    "            temperature=args['temperature'],\n",
    "            top_k=args['top_k'],\n",
    "            top_p=args['top_p'],\n",
    "            device=args['device'],\n",
    "        )\n",
    "        outputs = outputs[:, len(context_tokens):].tolist()\n",
    "        text_candidates = []\n",
    "        for out in outputs:\n",
    "            text = tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=False)\n",
    "            text = text[: text.find('<eos>')]\n",
    "            text_candidates.append(text)\n",
    "            print(text)\n",
    "        if args['prompt']:\n",
    "            break\n",
    "    return text_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elwood Generation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prompts = pd.read_csv('Elwood_Tester.txt', sep=\"<eos>\", header=None)\n",
    "prompts = prompts[0]\n",
    "prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptions pulled from other websites, with categories added\n",
    "\n",
    "prompts[0]= \"<bos> <category> Womens Clothing, Skirts <features> Structured crop top that features a textured woven Aztec fabric. \\\n",
    "The crop neckline sits high on the collar and fit slightly loose through the bust and body. \\\n",
    "The back of the crop features an open cross back design that is created through two panels weaved through each \\\n",
    "other then fall into a geometric peak at the hem. Crop is fully lined and is intended to pull-over the head for wear. \\\n",
    "The Summer Eyes Crop is designed to be worn with the Summer Swag Skirt. <title> Asilio Summer Swag Skirt \\t <long> \"\n",
    "\n",
    "prompts[1]= \"<bos> <category> Womens Clothing, jackets <features> Leopard is the new black, and velvet is the new velour. This jacket is a lovable feline \\\n",
    "accessory with quilted paneling and a cozy high neckline. Like any Unreal Fur garment, the Huff & Puff Jacket can be dressed \\\n",
    "up or down for any occasion where cold weather is a guest. The Huff & Puff Jacket features a concealed zipper, front pockets, \\\n",
    "and a high padded neckline. Our model is wearing an AU size Small. Available in extended sizes. \\\n",
    "Length: 62.5 CM Leopard print velvet outer shell Matte polyester lining Concealed zipper with button closures Long sleeves \\\n",
    "Elasticized cuffs <title> Unreal Fur Womens Huff & Puff Jacket Leopard \\t <long> \"\n",
    "\n",
    "prompts[2]= \"<bos> <category> Womens Clothing, Pants <features> Master contemporary-casual style in the Panther Pleat Pants, straight through the leg, with elastic waist \\\n",
    "and drawstring tie, this pieces answers weekend cool needs. Cut in an exclusive print with organic pleat effect that elevates this classic \\\n",
    "cut into a standout style. Length: 85cm 85% Rayon 15% Nylon. Dry textured finish Straight leg, cropped length Elastic waist with drawstring tie \\\n",
    "In house, Sydney HQ designed print, exclusive to THIRD FORM Roll hem Pleated fabric Our model is 175cm tall, wearing a size S/AU 8 and has a 65cm waist. \\\n",
    "<title> Third Form Pantha Pleat Pant Black Panther \\t <long> \"\n",
    "\n",
    "prompts[3] = \"<bos> <category> Womens Clothing and Accessories, Shoes <features> White sneaker. Leather upper and lining. Non-leather sole. Approx 5cm sole. \\\n",
    "Heel is approx 6cm. Size 8 is an EU39. This shoe runs true to size. From runway to real life, the sneaker trend is a thing and \\\n",
    "our ALIAS MAE Arlo Sneakers are no exception. Features a platform base and lace up across the vamp. <title> Alias Mae Arlo White Sneakers \\t <long> \"\n",
    "\n",
    "prompts[4]= \"<bos> <category> Womens Accessories, Jewelry, Earrings <features> Lucy Chain Hoops. The Lucy Chain hoops are a medium sized hoop in soft brushed finish. \\\n",
    "Perfect for everyday wear or layered up with other styled hoops.  <title> Jolie & Deen Womens Lucy Chain Hoops Earrings - Silver \\t <long> \"\n",
    "\n",
    "prompts[5]= \"<bos> <category> Baby & Kids, Kids Clothes, Girls Clothes, Shorts <features> Mini Rollers are a slim, low waist, relaxed fit denim short. Slim fit across hip. Raw dramatic curve through hem. Made from a rigid denim and features our signature button closure. Storm Buoy is a faded blue denim wash. Fabric: 100% cotton. \\\n",
    "Machine wash cold. Tumble dry to retain a soft hand feel & to maintain original fit. \\\n",
    "See the 'Lower Waist' measurement on our kids size chart to determine the best size. <title> Oneteaspoon Kids Rollers Denim Shorts Storm Buoy \\t <long> \"\n",
    "\n",
    "prompts[6]= \"<bos> <category> Mens Clothing and Accessories, Tops & T-shirts <features> Our Bass Pima Tee is a regular fit in 180g washed slub cotton jersey. We've created a relaxed fitted silhouette that's ideal for layering. Added with grinning stitch detailing through the back centre seam for a more unique feel. \\\n",
    "<title> Neuw Mens Bass Tee Military \\t <long>\"\n",
    "\n",
    "prompts[7]= \"<bos> <category> Mens Clothing and Accessories, Bottoms, Jeans <features> New from Nena And Pasadena this season, the Hellcat Elastic Ankle Jean is packed full of style and comfort. With classic moto styling, ribbed paneling and heavy distressing down front legs and tight elastic cuffs at both ankles, these pants are a must-have. Available now at Culture Kings while stocks last.\\\n",
    "- Moto-inspired long pants - Ribbed paneling and heavy distressing down front legs - Tight elastic cuffs at both ankles \\\n",
    "- Belt loops with button closure zip fly - 5-pocket design - 98% cotton, 2% elastane - True to size \\\n",
    "- Model is wearing size Medium/32 - Model usually wears - Tops: size Large, Pants: size Medium, Shoes: size 10 <title> Nena Pasadena Mens Hellcat Elastic Ankle Jeans Utah \\t <long> \"\n",
    "\n",
    "prompts[8]= \"<bos> <category> Mens Clothing and Accessories, Outerwear, Casual Jackets <features> The style is based on classic sherpa styles worn in Australia through the 70's,80's & 90's. \\\n",
    "The buttons used are shank not snap for extra strength. - Our Model is 189cm tall and wears a size: M \\\n",
    "- Composition: 100% Cotton, 100% Polyester Lining - Colour: Tan Cord - Long sleeves - Side pockets \\\n",
    "- Button through front <title> Rollas Mens Old Mate Sherpa Coat Tan Cord \\t <long> \"\n",
    "\n",
    "prompts[9]= \"<bos> <category> Mens Clothing and Accessories, athletic gear, bottoms <features> The Core Trackpant is a regular fit trackpant, featuring ribbed cuffs, \\\n",
    "side seam pockets and an Elwood branded print on thigh. These go-to trackies will become your season long favourite, \\\n",
    "made from 100% Cotton Fleece and finished with a garment soft-wash for a cosy feel, these tracksuit pants are sure to become a favourite <title> Elwood Mens Core Trackpant Dark Navy Mens \\t  <long> \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with the elwood descriptions instead of scraping\n",
    "\n",
    "prompts[0]= \"<bos> <category> Womens Clothing and Accessories, skirts <features> Standout highlight Aztec woven skirt by Asilio. \\\n",
    "The SUMMER SWAG SKIRT is a structured midi length skirt with cross front split styling and is made from a luxurious woven Aztec Fabric. \\\n",
    "Looks stunning when worn with a Black or White blouse and heels and is perfect for Spring Racing Festival or any highlight summery event. \\\n",
    "<title> Asilio Summer Swag Skirt \\t <long> \"\n",
    "\n",
    "prompts[1]= \"<bos> <category> Womens Clothing and Accessories, Jackets <features> Leopard is the new black, and velvet is the new velour. This jacket is a lovable feline accessory with quilted panelling, and a cozy high neckline. Like any Unreal Fur garment, the Huff & Puff Jacket can be dressed up or down for any occasion where cold weather is a guest. \\\n",
    "The Huff & Puff Jacket features a concealed zipper, front pockets, and a high padded neckline. \\\n",
    "Our model is wearing an AU size Small. Length: 62.5 CM Leopard print velvet outer shell Matte polyester lining \\\n",
    "Concealed zipper with button closures Long sleeves Elasticised cuffs <title> Unreal Fur Womens Huff & Puff Jacket Leopard \\t <long> \"\n",
    "\n",
    "prompts[2]= \"<bos> <category> Womens Clothing and Accessories, Pants <features> Australian designer pants in an exclusive Black White Panther Print from Third Form. \\\n",
    "Master contemporary-casual style in the Panther Pleat Pants, straight through the leg, with elastic waist and drawstring tie, this pieces answers weekend cool needs. \\\n",
    "Cut in an exclusive print with organic pleat effect that elevates this classic cut into a standout style. Length: 85cm \\\n",
    "85% Rayon 15% Nylon. Dry textured finish Straight leg, cropped length Elastic waist with drawstring tie In house, Sydney HQ designed print, exclusive to THIRD FORM \\\n",
    "Roll hem Pleated fabric Our model is 175cm tall, wearing a size S/AU 8 and has a 65cm waist. \\\n",
    "<title> Third Form Pantha Pleat Pant Black Panther \\t <long> \"\n",
    "\n",
    "prompts[3] = \"<bos> <category> Womens Clothing and Accessories, Shoes, Sneakers <features> White sneaker. Leather upper and lining. Non-leather sole. Approx 5cm sole. \\\n",
    "Heel is approx 6cm. Size 8 is an EU39. This shoe runs true to size. From runway to real life, the sneaker trend is a thing and \\\n",
    "our ALIAS MAE Arlo Sneakers are no exception. Features a platform base and lace up across the vamp. <title> Alias Mae Arlo White Sneakers \\t <long> \"\n",
    "\n",
    "prompts[4]= \"<bos> <category> Womens Accessories, Jewelry <features> Lucy Chain Hoops. The Lucy Chain hoops are a medium sized hoop in soft brushed finish. \\\n",
    "Perfect for everyday wear or layered up with other styled hoops.  <title> Jolie & Deen Womens Lucy Chain Hoops Earrings - Silver \\t <long> \"\n",
    "\n",
    "prompts[5]= \"<bos> <category> Baby & Kids, Kids Clothes, Girls Clothes, Shorts <features> HAND MADE DENIM STREETWEAR FOR KIDS by OneTeaspoon. \\\n",
    "Your Kids will be the coolest kids on the block in our all new OneTeaspoon Kids Collection. \\\n",
    "Mini Rollers are a slim, low waist, relaxed fit denim short. Slim fit across hip. Raw dramatic curve through hem. Made from a rigid denim and features our signature button closure. Storm Buoy is a faded blue denim wash. Fabric: 100% cotton. \\\n",
    "Hand Made and funky as you'd expect from OneTeaspoon. <title> Oneteaspoon Kids Rollers Denim Shorts Storm Buoy \\t <long> \"\n",
    "\n",
    "prompts[6]= \"<bos> <category> Mens Clothing and Accessories, Tops & T-shirts <features> Premium Pima Cotton Basic tee by Neuw ...a wardrobe essential item. \\\n",
    "The Bass Tee is a Neuw Denim classic and is cut to a regular fitting silhouette, providing an easy-wearing shape. \\\n",
    "It's crafted from quality Pima Cotton for a unique, soft and long lasting hand feel. And is finished with a classic crew neckline, \\\n",
    "fitted cap sleeves and double stitched side seams for durability. It's the perfect long-lasting piece for your Neuw wardrobe. \\\n",
    "Style it with your favourite denim wash and a leather or denim jacket for a timeless look. \\\n",
    "<title> Neuw Mens Bass Tee Military \\t <long>\"\n",
    "\n",
    "prompts[7]= \"<bos> <category> Mens Clothing and Accessories, Bottoms, Jeans <features> \\\n",
    "Introducing the new Top of the Range HELLCAT Pant by Nena Pasadena in KENTUCKY BLUE Denim, destined to be a best selling fit and style. \\\n",
    "The HELLCAT features a whole heap of distinctive style highlights incl : DROP CROTCH GUSSET \\\n",
    "SLIM ELASTIC CUFF ANKLES PIN TUCKED RIB DETAIL DISTRESSED KNEE OVERLAY 5 POCKET STYLING \\\n",
    "The Kentucky Blue colourway featured here, is the perfect washed mid blue denim that is always in demand, year in, year out, and looks great with any combination of tees and sneakers. \\\n",
    "<title> Nena Pasadena Mens Hellcat Elastic Ankle Jeans Utah \\t <long> \"\n",
    "\n",
    "prompts[8]= \"<bos> <category> Mens Clothing and Accessories, Outerwear, Casual Jackets <features> \\\n",
    "Mens Rollas Cordury and sherpa lined jacket.....ultra warm and a nod to the vintage styles of yesteryear. \\\n",
    "The style is based on classic sherpa styles worn in Australia through the 70's,80's & 90's. \\\n",
    "The buttons used are shank not snap for extra strength. Our Model is 189cm tall and wears a size: M \\\n",
    "- Composition: 100% Cotton, 100% Polyester Lining - Colour: Tan Cord - Long sleeves - Side pockets \\\n",
    "- Button through front <title> Rollas Mens Old Mate Sherpa Coat Tan Cord \\t <long> \"\n",
    "\n",
    "prompts[9]= \"<bos> <category> Mens Clothing and Accessories, athletic gear, bottoms <features> \\\n",
    "Just in by Elwood.. The Core trackpant. This is a regular fit with ribbed cuff, side seam pockets and soft plastisol thigh print. \\\n",
    "Made from 100% Cotton fleece and finished with a garment soft wash. <title> Elwood Mens Core Trackpant Dark Navy Mens \\t  <long> \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts= prompts[:10]\n",
    "prompts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerlist = []\n",
    "for i in range(len(prompts)):\n",
    "    inputs= prompts[i]\n",
    "    testerlist.append([inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args[\"model_type\"]='gpt2'\n",
    "args['model_name_or_path']= 'OldIconicOutput/checkpoint3'\n",
    "args['prompt']=\"\"\n",
    "args['padding_text']= ''#WHAT'S MY PADDING TEXT?\n",
    "args['length']=200\n",
    "args['temperature']=1.0\n",
    "args['top_k']=0\n",
    "args['top_p']=0.9\n",
    "args['no_cuda']=False\n",
    "args['seed']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elwood test block\n",
    "\n",
    "with open(\"Elwood_Tester.txt\", \"w\") as text_file:\n",
    "    for i in range(len(testerlist)):\n",
    "        args['prompt']=testerlist[i][0]\n",
    "        prompt = args['prompt']\n",
    "        line = f\"Prompt:\\n{prompt}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        for o in range(41, 45):\n",
    "            args['seed']=o\n",
    "            generated = generate(args)\n",
    "            line = f\"\\nGenerated description {o}:\\n{generated}\\n\\n\"\n",
    "            print(line, file=text_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out TF-IDF scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.',\n",
    "'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"Elwood_Multi_TFIDF_Test.txt\", \"w\") as text_file:\n",
    "    for i in range(len(testerlist)):\n",
    "        descriptions = []\n",
    "        args['prompt']=testerlist[i][0]\n",
    "        prompt = args['prompt']\n",
    "        descriptions.append(prompt)\n",
    "        line = f\"Prompt:\\n{prompt}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        for o in range(20):\n",
    "            args['seed']=o\n",
    "            generated = generate(args)\n",
    "            descriptions.append(generated[0])\n",
    "            print(o)\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf = vectorizer.fit_transform(descriptions)                                                                                                                                                                                                                    \n",
    "        pairwise_similarity = tfidf * tfidf.T \n",
    "        arr = pairwise_similarity.toarray()\n",
    "        np.fill_diagonal(arr, np.nan)\n",
    "    \n",
    "        input_idx = descriptions.index(prompt)\n",
    "        result_idx = np.nanargmax(arr[input_idx])\n",
    "    \n",
    "        generated = descriptions[result_idx]\n",
    "        line = f\"\\nGenerated description 1:\\n{generated}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        arr[input_idx,result_idx] = np.nan\n",
    "        \n",
    "        result_idx = np.nanargmax(arr[input_idx])\n",
    "        generated = descriptions[result_idx]\n",
    "        line = f\"\\nGenerated description 2:\\n{generated}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        arr[input_idx,result_idx] = np.nan\n",
    "        \n",
    "        result_idx = np.nanargmax(arr[input_idx])\n",
    "        generated = descriptions[result_idx]\n",
    "        line = f\"\\nGenerated description 3:\\n{generated}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        arr[input_idx,result_idx] = np.nan\n",
    "        \n",
    "        result_idx = np.nanargmax(arr[input_idx])\n",
    "        generated = descriptions[result_idx]\n",
    "        line = f\"\\nGenerated description 4:\\n{generated}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        arr[input_idx,result_idx] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    <bos> <category> Women/Shoes/Sandals <features...\n",
       "1    <bos> <category> Women/Shoes/Sandals <features...\n",
       "2    <bos> <category> Women/Shoes/Boots <features> ...\n",
       "3    <bos> <category> Women/Shoes/Sandals <features...\n",
       "4    <bos> <category> Women/Shoes/Flats <features> ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prompts = pd.read_csv('IconicShoesTester.txt', sep=\"<eos>\", header=None)\n",
    "prompts = prompts[0]\n",
    "prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerlist = []\n",
    "for i in range(len(prompts)):\n",
    "    inputs, description = prompts[i].split('<description>')\n",
    "    testerlist.append([inputs + ' <description> ', description])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<bos> <category> Women/Shoes/Sandals <features>  Suede upper leather lined- Black hue- Open almond toe- Fixed vamp strap- Crossed midfoot straps- Pocket heel- Braided espadrille midsole- Crepe rubber sole- Slip-on design- 2.8cm heel <brand> OFFICE <model> Hallie \\t <description> ',\n",
       " ' The Hallie sandals from OFFICE exude a sanguine nonchalance with their crossed suede strapping and woven espadrille midsole ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testerlist[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "\n",
    "args[\"model_type\"]='gpt2'\n",
    "args['model_name_or_path']= 'IconicShoesOutput/checkpoint-1'\n",
    "args['prompt']=\"\"\n",
    "args['padding_text']= ''#WHAT'S MY PADDING TEXT?\n",
    "args['length']=200\n",
    "args['temperature']=1.0\n",
    "args['top_k']=0\n",
    "args['top_p']=0.9\n",
    "args['no_cuda']=False\n",
    "args['seed']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check multiple checkpoints\n",
    "checkpoints = ['IconicShoesOutput/checkpoint-1', 'IconicShoesOutput/checkpoint-15', 'IconicShoesOutput/checkpoint-2', 'IconicShoesOutput/checkpoint-25', 'IconicShoesOutput/checkpoint-30000', 'IconicShoesOutput/checkpoint-35000', 'IconicShoesOutput/checkpoint-40000', 'IconicShoesOutput/checkpoint-45000', 'IconicShoesOutput/checkpoint-5000', 'IconicShoesOutput/checkpoint-50000', 'IconicShoesOutput/checkpoint-55000', 'IconicShoesOutput/checkpoint-60000', 'IconicShoesOutput/checkpoint-65000', 'IconicShoesOutput/checkpoint-70000', 'IconicShoesOutput/checkpoint-75000', 'IconicShoesOutput/checkpoint-80000', 'IconicShoesOutput/checkpoint-85000', 'IconicShoesOutput/checkpoint-90000', 'IconicShoesOutput/checkpoint-95000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2020 21:09:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/jupyter/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "02/05/2020 21:09:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/jupyter/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "02/05/2020 21:09:27 - INFO - transformers.configuration_utils -   loading configuration file IconicShoesOutput/checkpoint-1/config.json\n",
      "02/05/2020 21:09:27 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/05/2020 21:09:27 - INFO - transformers.modeling_utils -   loading weights file IconicShoesOutput/checkpoint-1/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f01f87da6b47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf'\\n\\n<generated>{i}: \\n{generated[0]}\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-9c8d12f19780>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_position_embeddings\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'length'"
     ]
    }
   ],
   "source": [
    "with open(\"IconicCheckpointTest.txt\", \"w\") as text_file:\n",
    "    for point in checkpoints:\n",
    "        args['prompt']= testerlist[0]\n",
    "        args['model_name_or_path']= point\n",
    "        line = f'\\n{point}\\n<prompt> \\n{testerlist[0][0]}'\n",
    "        for i in range(40,44):\n",
    "            args['seed']=i\n",
    "            generated = generate(args)\n",
    "            line += f'\\n\\n<generated>{i}: \\n{generated[0]}\\n\\n'\n",
    "        print(line, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tester in testerlist:\n",
    "    args['prompt']= tester[0]\n",
    "#         line = f'<prompt> \\n{args['prompt']} \\n\\n<actual description> \\n{tester[1]}'\n",
    "    for i in range(40,44):\n",
    "        args['seed']=i\n",
    "        generated = generate(args)\n",
    "        print(generated[0])\n",
    "#             line += f'\\n\\n<generated>{i}: \\n{generated}\\n\\n'\n",
    "#         print(line, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"IconicTestRun.txt\", \"w\") as text_file:\n",
    "    for tester in testerlist:\n",
    "        args['prompt']= tester[0]\n",
    "        line = f'<prompt> \\n{tester[0]} \\n\\n<actual description> \\n{tester[1]}'\n",
    "        for i in range(40,44):\n",
    "            args['seed']=i\n",
    "            generated = generate(args)\n",
    "            line += f'\\n\\n<generated>{i}: \\n{generated[0]}\\n\\n'\n",
    "        print(line, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "with open(\"Iconic_TFIDF_Test.txt\", \"w\") as text_file:\n",
    "    for i in range(len(testerlist)):\n",
    "        descriptions = []\n",
    "        args['prompt']=testerlist[i][0]\n",
    "        prompt = args['prompt']\n",
    "        descriptions.append(prompt)\n",
    "        line = f\"Prompt:\\n{prompt}\\n\\n\"\n",
    "        print(line, file=text_file)\n",
    "        for o in range(15):\n",
    "            args['seed']=o\n",
    "            generated = generate(args)\n",
    "            descriptions.append(generated[0])\n",
    "            print(o)\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf = vectorizer.fit_transform(descriptions)                                                                                                                                                                                                                    \n",
    "        pairwise_similarity = tfidf * tfidf.T \n",
    "        arr = pairwise_similarity.toarray()\n",
    "        np.fill_diagonal(arr, np.nan)\n",
    "    \n",
    "        input_idx = descriptions.index(prompt)\n",
    "        result_idx = np.nanargmax(arr[input_idx])\n",
    "        generated = descriptions[result_idx]\n",
    "        line = f\"\\nGenerated description {o}:\\n{generated}\\n\\n\"\n",
    "        print(line, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
